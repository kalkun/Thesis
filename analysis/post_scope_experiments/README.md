### Post scope experiments

In here we have experiments that go beyond the initial scope
of trying to imitate the method of UCLA and reproducing their
results.

##### Implemented
The experiments that are thought to be here, are represented
by inidividual notebooks with the following ideas:

- [ ] Using initial training with search engine results
- [ ] Other base network than resnet e.g. Xception or InceptionResNet
- [X] Changing output neuron (linear)
- [X] Add fully connected layers at the end + dropout
- [X] Reduce resizing (data augmentation)
- [X] Optimizers (using Adam instead of SGD)
- [X] Freeze the first _m_ layers (First 5 conv layers)


These experiments are generally in two different categories:
 1. Experiments based on `UCLA_model_UCLA_dataset_without_visual_attributes.ipynb`
 2. Experiments that are a different approach to the setup.


##### Running

- [ ] Using initial training with search engine results
- [ ] Other base network than resnet e.g. Xception or InceptionResNet
- [ ] Changing output neuron (linear)
- [ ] Add fully connected layers at the end + dropout
- [X] Reduce resizing (data augmentation)
- [ ] Optimizers (using Adam instead of SGD)
- [X] Freeze the first _m_ layers (First 5 conv layers)

##### Completed

- [ ] Using initial training with search engine results
- [ ] Other base network than resnet e.g. Xception or InceptionResNet
- [ ] Changing output neuron (linear)
- [X] Add fully connected layers at the end + dropout
- [ ] Reduce resizing (data augmentation)
- [ ] Optimizers (using Adam instead of SGD)
- [ ] Freeze the first _m_ layers (First 5 conv layers)
