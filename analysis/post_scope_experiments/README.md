### Post scope experiments

In here we have experiments that go beyond the initial scope
of trying to imitate the method of UCLA and reproducing their
results.

The experiments that are thought to be here, are represented
by inidividual notebooks with the following ideas:

- Using initial training with search engine results
- Other base network than resnet e.g. Xception or InceptionResNet
- Changing output neuron (linear)
- Add fully connected layers at the end + dropout
- Reduce resizing (data augmentation)
- Optimizers (using Adam instead of SGD)
- Freeze the first _m_ layers
